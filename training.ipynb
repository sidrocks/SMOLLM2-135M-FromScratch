{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Training SmolLM2-135M\n",
                "\n",
                "This notebook demonstrates how to train the custom `SmolLM2` model (imported from `model.py`) on the Wikitext dataset. It includes:\n",
                "1. Training for 5000 steps.\n",
                "2. Generating text every 500 steps.\n",
                "3. Saving a checkpoint.\n",
                "4. Resuming training from the checkpoint."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install PyTorch with CUDA support for Windows (assuming CUDA 12.1)\n",
                "# If this fails or you have a different CUDA version, check https://pytorch.org/get-started/locally/\n",
                "!pip uninstall -y torch torchvision\n",
                "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
                "!pip install transformers datasets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> **IMPORTANT**: After running the cell above, you **MUST** restart the Jupyter Kernel for the changes to take effect. Go to **Kernel > Restart Kernel** in the menu."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "from transformers import AutoTokenizer, AutoConfig\n",
                "from datasets import load_dataset\n",
                "from model import SmolLM2  # Import our custom model\n",
                "import os\n",
                "\n",
                "print(f\"PyTorch Version: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "if device == \"cpu\":\n",
                "    print(\"WARNING: You are running on CPU. Training will be very slow. Please ensure you have a GPU and the correct PyTorch version installed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Model and Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Custom Tokenizer\n",
                "tokenizer_path = \"./custom_tokenizer\"\n",
                "\n",
                "# Check if custom tokenizer exists\n",
                "if os.path.exists(tokenizer_path):\n",
                "    print(f\"Loading custom tokenizer from {tokenizer_path}\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
                "else:\n",
                "    print(f\"Custom tokenizer not found at {tokenizer_path}. Please run train_tokenizer.ipynb first.\")\n",
                "    # Fallback to default for safety, or raise error. \n",
                "    # Assuming user wants custom, we should probably raise error or warn heavily.\n",
                "    # For this flow, let's fallback but warn.\n",
                "    print(\"Falling back to default tokenizer (HuggingFaceTB/SmolLM2-135M)...\")\n",
                "    model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "\n",
                "# Ensure pad_token is set\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    print(f\"Padding token set to: {tokenizer.pad_token}\")\n",
                "\n",
                "model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
                "config = AutoConfig.from_pretrained(model_id)\n",
                "\n",
                "# Update config vocab size to match tokenizer\n",
                "config.vocab_size = len(tokenizer)\n",
                "print(f\"Model vocab size updated to: {config.vocab_size}\")\n",
                "\n",
                "# Initialize model from scratch\n",
                "model = SmolLM2(config).to(device)\n",
                "print(\"Model initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prepare Dataset (Chunked)\n",
                "We concatenate text and split into chunks to allow the model to learn context across lines."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset by reading the full file to preserve newlines and formatting\n",
                "with open(\"input-1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
                "    full_text = f.read()\n",
                "\n",
                "from datasets import Dataset\n",
                "dataset = Dataset.from_dict({\"text\": [full_text]})\n",
                "\n",
                "# Double check tokenizer padding\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "block_size = 256 # Context window size\n",
                "\n",
                "def group_texts(examples):\n",
                "    # Concatenate all texts\n",
                "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
                "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
                "    \n",
                "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
                "    if total_length >= block_size:\n",
                "        total_length = (total_length // block_size) * block_size\n",
                "        \n",
                "    # Split by chunks of max_len\n",
                "    result = {\n",
                "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
                "        for k, t in concatenated_examples.items()\n",
                "    }\n",
                "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
                "    return result\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(examples[\"text\"])\n",
                "\n",
                "# Tokenize all text\n",
                "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
                "\n",
                "# Group into chunks\n",
                "lm_dataset = tokenized_dataset.map(\n",
                "    group_texts,\n",
                "    batched=True,\n",
                "    batch_size=1000,\n",
                "    # num_proc=4, # Disabled to avoid Windows multiprocessing 'spawn' issues with global variables\n",
                ")\n",
                "\n",
                "lm_dataset = lm_dataset.with_format(\"torch\")\n",
                "\n",
                "# Create dataloader\n",
                "train_dataloader = DataLoader(lm_dataset, batch_size=4, shuffle=True)\n",
                "print(f\"Dataset prepared. Number of chunks: {len(lm_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optimization: Enable TF32 for faster matrix multiplications on Ampere+ GPUs\n",
                "torch.backends.cuda.matmul.allow_tf32 = True\n",
                "torch.backends.cudnn.allow_tf32 = True\n",
                "\n",
                "# Optimization: Use torch.compile (PyTorch 2.0+)\n",
                "# Windows support for torch.compile can be tricky, so we wrap it in a try-except or check os\n",
                "if os.name != 'nt': # torch.compile often has issues on Windows currently, skipping for safety or try 'inductor'\n",
                "     print(\"Compiling model with torch.compile...\")\n",
                "     model = torch.compile(model)\n",
                "else:\n",
                "    print(\"Skipping torch.compile on Windows to avoid potential compatibility issues.\")\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, fused=True if torch.cuda.is_available() else False)\n",
                "loss_fn = torch.nn.CrossEntropyLoss()\n",
                "\n",
                "# Optimization: Mixed Precision Training\n",
                "scaler = torch.cuda.amp.GradScaler()\n",
                "\n",
                "def generate_text(model, tokenizer, prompt=\"The meaning of life is\", max_new_tokens=50, temperature=0.7, top_k=50):\n",
                "    model.eval()\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                "    input_ids = inputs.input_ids\n",
                "    \n",
                "    for _ in range(max_new_tokens):\n",
                "        with torch.no_grad():\n",
                "            # Auto-cast is not strictly necessary for inference but can speed it up\n",
                "            with torch.cuda.amp.autocast():\n",
                "                logits = model(input_ids)\n",
                "            next_token_logits = logits[:, -1, :] / temperature\n",
                "            \n",
                "            # Top-k sampling\n",
                "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
                "            probs = torch.nn.functional.softmax(top_k_logits, dim=-1)\n",
                "            next_token_index = torch.multinomial(probs, num_samples=1)\n",
                "            next_token = top_k_indices.gather(-1, next_token_index)\n",
                "            \n",
                "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
                "            \n",
                "            if next_token.item() == tokenizer.eos_token_id:\n",
                "                break\n",
                "            \n",
                "    print(f\"Generated: {tokenizer.decode(input_ids[0], skip_special_tokens=True)}\")\n",
                "    model.train()\n",
                "\n",
                "steps = 0\n",
                "max_steps = 5000\n",
                "save_path = \"checkpoint_5000.pt\"\n",
                "\n",
                "model.train()\n",
                "print(\"Starting training...\")\n",
                "\n",
                "# Loop indefinitely until max_steps is reached\n",
                "while steps < max_steps:\n",
                "    for batch in train_dataloader:\n",
                "        if steps >= max_steps:\n",
                "            break\n",
                "            \n",
                "        input_ids = batch[\"input_ids\"].to(device)\n",
                "        # Shift labels for causal LM\n",
                "        labels = input_ids.clone()\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Optimization: Mixed Precision Context\n",
                "        with torch.cuda.amp.autocast():\n",
                "            logits = model(input_ids)\n",
                "            \n",
                "            # Shift so that tokens < n predict n\n",
                "            shift_logits = logits[..., :-1, :].contiguous()\n",
                "            shift_labels = labels[..., 1:].contiguous()\n",
                "            \n",
                "            loss = loss_fn(shift_logits.view(-1, config.vocab_size), shift_labels.view(-1))\n",
                "        \n",
                "        # Optimization: Scaled Backward Pass\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        steps += 1\n",
                "        \n",
                "        if steps % 100 == 0:\n",
                "            print(f\"Step {steps}: Loss {loss.item()}\")\n",
                "            \n",
                "        if steps % 500 == 0:\n",
                "            print(f\"\\n--- Step {steps} Generation ---\")\n",
                "            generate_text(model, tokenizer)\n",
                "            print(\"-----------------------------\\n\")\n",
                "\n",
                "# Save Checkpoint\n",
                "torch.save(model.state_dict(), save_path)\n",
                "print(f\"Checkpoint saved to {save_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Resume Training\n",
                "Now we simulate stopping and resuming by loading the checkpoint and training for 50 more steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Resuming training...\")\n",
                "model = SmolLM2(config).to(device)\n",
                "model.load_state_dict(torch.load(save_path))\n",
                "model.train()\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
                "\n",
                "extra_steps = 50\n",
                "current_step = 0\n",
                "\n",
                "# Re-create dataloader for resume (in real scenario you'd want to skip already seen data)\n",
                "# Note: In this simple example, we restart the dataloader from the beginning.\n",
                "# In a real resume scenario, you'd want to save the dataloader state or skip 'steps' batches.\n",
                "train_dataloader_resume = DataLoader(lm_dataset, batch_size=4, shuffle=True)\n",
                "\n",
                "for batch in train_dataloader_resume:\n",
                "    if current_step >= extra_steps:\n",
                "        break\n",
                "        \n",
                "    input_ids = batch[\"input_ids\"].to(device)\n",
                "    labels = input_ids.clone()\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    logits = model(input_ids)\n",
                "    \n",
                "    shift_logits = logits[..., :-1, :].contiguous()\n",
                "    shift_labels = labels[..., 1:].contiguous()\n",
                "    \n",
                "    loss = loss_fn(shift_logits.view(-1, config.vocab_size), shift_labels.view(-1))\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    current_step += 1\n",
                "    if current_step % 10 == 0:\n",
                "        print(f\"Resume Step {current_step}: Loss {loss.item()}\")\n",
                "\n",
                "print(f\"\\n--- Step {steps + extra_steps} Generation ---\")\n",
                "generate_text(model, tokenizer, prompt=\"First Citizen\", max_new_tokens=1024)\n",
                "print(\"-----------------------------\\n\")\n",
                "\n",
                "print(\"Resumed training completed.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}