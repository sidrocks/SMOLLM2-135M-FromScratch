{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Training SmolLM2-135M\n",
                "\n",
                "This notebook demonstrates how to train the custom `SmolLM2` model (imported from `model.py`) on custom dataset. It includes:\n",
                "1. Training for 5000 steps.\n",
                "2. Generating text every 500 steps.\n",
                "3. Saving a checkpoint.\n",
                "4. Resuming training from the checkpoint."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install PyTorch with CUDA support for Windows (assuming CUDA 12.1)\n",
                "# If this fails or you have a different CUDA version, check https://pytorch.org/get-started/locally/\n",
                "!pip uninstall -y torch torchvision\n",
                "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
                "!pip install transformers datasets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> **IMPORTANT**: After running the cell above, you **MUST** restart the Jupyter Kernel for the changes to take effect. Go to **Kernel > Restart Kernel** in the menu."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch Version: 2.5.1+cu121\n",
                        "CUDA Available: True\n",
                        "CUDA Device: NVIDIA RTX 5000 Ada Generation Laptop GPU\n",
                        "CUDA Version: 12.1\n",
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "from transformers import AutoTokenizer, AutoConfig\n",
                "from datasets import load_dataset\n",
                "from model import SmolLM2  # Import our custom model\n",
                "import os\n",
                "\n",
                "print(f\"PyTorch Version: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "if device == \"cpu\":\n",
                "    print(\"WARNING: You are running on CPU. Training will be very slow. Please ensure you have a GPU and the correct PyTorch version installed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Model and Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading custom tokenizer from ./custom_tokenizer\n",
                        "Model vocab size updated to: 5001\n",
                        "Model initialized.\n"
                    ]
                }
            ],
            "source": [
                "# Load Custom Tokenizer\n",
                "tokenizer_path = \"./custom_tokenizer\"\n",
                "\n",
                "# Check if custom tokenizer exists\n",
                "if os.path.exists(tokenizer_path):\n",
                "    print(f\"Loading custom tokenizer from {tokenizer_path}\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
                "else:\n",
                "    print(f\"Custom tokenizer not found at {tokenizer_path}. Please run train_tokenizer.ipynb first.\")\n",
                "    # Fallback to default for safety, or raise error. \n",
                "    # Assuming user wants custom, we should probably raise error or warn heavily.\n",
                "    # For this flow, let's fallback but warn.\n",
                "    print(\"Falling back to default tokenizer (HuggingFaceTB/SmolLM2-135M)...\")\n",
                "    model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "\n",
                "# Ensure pad_token is set\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    print(f\"Padding token set to: {tokenizer.pad_token}\")\n",
                "\n",
                "model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
                "config = AutoConfig.from_pretrained(model_id)\n",
                "\n",
                "# Update config vocab size to match tokenizer\n",
                "config.vocab_size = len(tokenizer)\n",
                "print(f\"Model vocab size updated to: {config.vocab_size}\")\n",
                "\n",
                "# Initialize model from scratch\n",
                "model = SmolLM2(config).to(device)\n",
                "print(\"Model initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prepare Dataset (Chunked)\n",
                "We concatenate text and split into chunks to allow the model to learn context across lines."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "dd6f943532b9465fa9965a307f337e84",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7052e7e76dda438480a5b27756b52e99",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset prepared. Number of chunks: 1308\n"
                    ]
                }
            ],
            "source": [
                "# Load dataset by reading the full file to preserve newlines and formatting\n",
                "with open(\"input-1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
                "    full_text = f.read()\n",
                "\n",
                "from datasets import Dataset\n",
                "dataset = Dataset.from_dict({\"text\": [full_text]})\n",
                "\n",
                "# Double check tokenizer padding\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "block_size = 256 # Context window size\n",
                "\n",
                "def group_texts(examples):\n",
                "    # Concatenate all texts\n",
                "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
                "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
                "    \n",
                "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
                "    if total_length >= block_size:\n",
                "        total_length = (total_length // block_size) * block_size\n",
                "        \n",
                "    # Split by chunks of max_len\n",
                "    result = {\n",
                "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
                "        for k, t in concatenated_examples.items()\n",
                "    }\n",
                "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
                "    return result\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(examples[\"text\"])\n",
                "\n",
                "# Tokenize all text\n",
                "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
                "\n",
                "# Group into chunks\n",
                "lm_dataset = tokenized_dataset.map(\n",
                "    group_texts,\n",
                "    batched=True,\n",
                "    batch_size=1000,\n",
                "    # num_proc=4, # Disabled to avoid Windows multiprocessing 'spawn' issues with global variables\n",
                ")\n",
                "\n",
                "lm_dataset = lm_dataset.with_format(\"torch\")\n",
                "\n",
                "# Create dataloader\n",
                "train_dataloader = DataLoader(lm_dataset, batch_size=4, shuffle=True)\n",
                "print(f\"Dataset prepared. Number of chunks: {len(lm_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Skipping torch.compile on Windows to avoid potential compatibility issues.\n",
                        "Starting training...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\sidhe\\AppData\\Local\\Temp\\ipykernel_23008\\2461595262.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
                        "  scaler = torch.cuda.amp.GradScaler()\n",
                        "C:\\Users\\sidhe\\AppData\\Local\\Temp\\ipykernel_23008\\2461595262.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
                        "  with torch.cuda.amp.autocast():\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Step 100: Loss 5.271072864532471\n",
                        "Step 200: Loss 5.146054267883301\n",
                        "Step 300: Loss 4.636077404022217\n",
                        "Step 400: Loss 4.05342960357666\n",
                        "Step 500: Loss 4.514355182647705\n",
                        "\n",
                        "--- Step 500 Generation ---\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\sidhe\\AppData\\Local\\Temp\\ipykernel_23008\\2461595262.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
                        "  with torch.cuda.amp.autocast():\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generated: The meaning of life is\n",
                        "The people; and therefore the very royal soldiers,\n",
                        "The noble duke, I'll make you both the people.\n",
                        "\n",
                        "PETRUCHIO:\n",
                        "Well, I must:\n",
                        "What you have some mil'dly?\n",
                        "\n",
                        "GLOUCESTER:\n",
                        "What\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 600: Loss 3.852552652359009\n",
                        "Step 700: Loss 3.67338228225708\n",
                        "Step 800: Loss 3.858555316925049\n",
                        "Step 900: Loss 3.88484263420105\n",
                        "Step 1000: Loss 3.3133487701416016\n",
                        "\n",
                        "--- Step 1000 Generation ---\n",
                        "Generated: The meaning of life is\n",
                        "drumbling; and in the middle of love\n",
                        "of I am the end of the hand: I have sworn\n",
                        "without you have done.\n",
                        "\n",
                        "LEONTES:\n",
                        "I pray you, sir, I'll swear\n",
                        "To say you\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 1100: Loss 3.7228200435638428\n",
                        "Step 1200: Loss 3.4301235675811768\n",
                        "Step 1300: Loss 3.7488114833831787\n",
                        "Step 1400: Loss 3.0603678226470947\n",
                        "Step 1500: Loss 3.1305861473083496\n",
                        "\n",
                        "--- Step 1500 Generation ---\n",
                        "Generated: The meaning of life is it not;\n",
                        "And she, for all the fairest of this earth;\n",
                        "And, by the other of heaven she's love,\n",
                        "Persuitute it is not be married.\n",
                        "\n",
                        "KING HENRY VI:\n",
                        "You bid me entreat\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 1600: Loss 2.9551541805267334\n",
                        "Step 1700: Loss 2.3346869945526123\n",
                        "Step 1800: Loss 2.3356552124023438\n",
                        "Step 1900: Loss 2.467982769012451\n",
                        "Step 2000: Loss 1.4448952674865723\n",
                        "\n",
                        "--- Step 2000 Generation ---\n",
                        "Generated: The meaning of life is full of Naples.\n",
                        "\n",
                        "KING RICHARD III:\n",
                        "Methoughts it is in love tooth.\n",
                        "\n",
                        "DERBY:\n",
                        "No; but every tale hath he of a horse.\n",
                        "\n",
                        "KING RICHARD III:\n",
                        "Think, I would have had\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 2100: Loss 1.4267183542251587\n",
                        "Step 2200: Loss 1.770742654800415\n",
                        "Step 2300: Loss 0.814985454082489\n",
                        "Step 2400: Loss 0.7272887229919434\n",
                        "Step 2500: Loss 0.8367995619773865\n",
                        "\n",
                        "--- Step 2500 Generation ---\n",
                        "Generated: The meaning of life is ta'en out of let's\n",
                        "bestilence this out-virt lady and my good\n",
                        "fortune, within my state was not all much abused; for\n",
                        "you are but infanted: but then all poverse\n",
                        "\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 2600: Loss 1.0043246746063232\n",
                        "Step 2700: Loss 0.259679913520813\n",
                        "Step 2800: Loss 0.2687181532382965\n",
                        "Step 2900: Loss 0.3258618414402008\n",
                        "Step 3000: Loss 0.12177103757858276\n",
                        "\n",
                        "--- Step 3000 Generation ---\n",
                        "Generated: The meaning of life is procure\n",
                        "Than thou shalt be; and, ere I do not say.\n",
                        "\n",
                        "KING RICHARD II:\n",
                        "This is no mortal drunibal:\n",
                        "This is no way to be sent for from him\n",
                        "More than he is, yet\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 3100: Loss 0.09513700753450394\n",
                        "Step 3200: Loss 0.08790317177772522\n",
                        "Step 3300: Loss 0.04893791675567627\n",
                        "Step 3400: Loss 0.0646422952413559\n",
                        "Step 3500: Loss 0.06223556771874428\n",
                        "\n",
                        "--- Step 3500 Generation ---\n",
                        "Generated: The meaning of life is now a son,\n",
                        "To be seen in him that acts at his,\n",
                        "As were he, I would set you on him,\n",
                        "As he is proud to be his worthless.\n",
                        "\n",
                        "Lord Mayor:\n",
                        "Hail how he\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 3600: Loss 0.054624490439891815\n",
                        "Step 3700: Loss 0.05200936645269394\n",
                        "Step 3800: Loss 0.06808382272720337\n",
                        "Step 3900: Loss 0.07332677394151688\n",
                        "Step 4000: Loss 0.08571958541870117\n",
                        "\n",
                        "--- Step 4000 Generation ---\n",
                        "Generated: The meaning of life is full of person.\n",
                        "\n",
                        "HENRY BOLINGBROKE:\n",
                        "You are pardon, my lord, I know my:\n",
                        "Not yet my grief hath praise my compare,\n",
                        "To fight against the fair degree and the fearful land\n",
                        "To this extremity and\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 4100: Loss 0.08139943331480026\n",
                        "Step 4200: Loss 0.1808193176984787\n",
                        "Step 4300: Loss 0.16262556612491608\n",
                        "Step 4400: Loss 0.21453623473644257\n",
                        "Step 4500: Loss 0.3636895418167114\n",
                        "\n",
                        "--- Step 4500 Generation ---\n",
                        "Generated: The meaning of life is the house of you.\n",
                        "\n",
                        "Nurse:\n",
                        "O brother, your young lady, for yourself to him.\n",
                        "\n",
                        "JULIET:\n",
                        "Marry, and grant it should not be so yet.\n",
                        "\n",
                        "ROMEO:\n",
                        "Peace, peace! help, peace\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 4600: Loss 0.18211005628108978\n",
                        "Step 4700: Loss 0.22678734362125397\n",
                        "Step 4800: Loss 0.23497304320335388\n",
                        "Step 4900: Loss 0.22399206459522247\n",
                        "Step 5000: Loss 0.07477153837680817\n",
                        "\n",
                        "--- Step 5000 Generation ---\n",
                        "Generated: The meaning of life is scarce any\n",
                        "Than twentyh made it; who, in a kind of course,\n",
                        "that they shall know, as if they had been\n",
                        "A thing for that danger, my sovereign.\n",
                        "\n",
                        "HENRY BOLINGBROKE:\n",
                        "Call forth Bel and set\n",
                        "-----------------------------\n",
                        "\n",
                        "Checkpoint saved to checkpoint_5000.pt\n"
                    ]
                }
            ],
            "source": [
                "# Optimization: Enable TF32 for faster matrix multiplications on Ampere+ GPUs\n",
                "torch.backends.cuda.matmul.allow_tf32 = True\n",
                "torch.backends.cudnn.allow_tf32 = True\n",
                "\n",
                "# Optimization: Use torch.compile (PyTorch 2.0+)\n",
                "# Windows support for torch.compile can be tricky, so we wrap it in a try-except or check os\n",
                "if os.name != 'nt': # torch.compile often has issues on Windows currently, skipping for safety or try 'inductor'\n",
                "     print(\"Compiling model with torch.compile...\")\n",
                "     model = torch.compile(model)\n",
                "else:\n",
                "    print(\"Skipping torch.compile on Windows to avoid potential compatibility issues.\")\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, fused=True if torch.cuda.is_available() else False)\n",
                "loss_fn = torch.nn.CrossEntropyLoss()\n",
                "\n",
                "# Optimization: Mixed Precision Training\n",
                "scaler = torch.cuda.amp.GradScaler()\n",
                "\n",
                "def generate_text(model, tokenizer, prompt=\"The meaning of life is\", max_new_tokens=50, temperature=0.7, top_k=50):\n",
                "    model.eval()\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                "    input_ids = inputs.input_ids\n",
                "    \n",
                "    for _ in range(max_new_tokens):\n",
                "        with torch.no_grad():\n",
                "            # Auto-cast is not strictly necessary for inference but can speed it up\n",
                "            with torch.cuda.amp.autocast():\n",
                "                logits = model(input_ids)\n",
                "            next_token_logits = logits[:, -1, :] / temperature\n",
                "            \n",
                "            # Top-k sampling\n",
                "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
                "            probs = torch.nn.functional.softmax(top_k_logits, dim=-1)\n",
                "            next_token_index = torch.multinomial(probs, num_samples=1)\n",
                "            next_token = top_k_indices.gather(-1, next_token_index)\n",
                "            \n",
                "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
                "            \n",
                "            if next_token.item() == tokenizer.eos_token_id:\n",
                "                break\n",
                "            \n",
                "    print(f\"Generated: {tokenizer.decode(input_ids[0], skip_special_tokens=True)}\")\n",
                "    model.train()\n",
                "\n",
                "steps = 0\n",
                "max_steps = 5000\n",
                "save_path = \"checkpoint_5000.pt\"\n",
                "\n",
                "model.train()\n",
                "print(\"Starting training...\")\n",
                "\n",
                "# Loop indefinitely until max_steps is reached\n",
                "while steps < max_steps:\n",
                "    for batch in train_dataloader:\n",
                "        if steps >= max_steps:\n",
                "            break\n",
                "            \n",
                "        input_ids = batch[\"input_ids\"].to(device)\n",
                "        # Shift labels for causal LM\n",
                "        labels = input_ids.clone()\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Optimization: Mixed Precision Context\n",
                "        with torch.cuda.amp.autocast():\n",
                "            logits = model(input_ids)\n",
                "            \n",
                "            # Shift so that tokens < n predict n\n",
                "            shift_logits = logits[..., :-1, :].contiguous()\n",
                "            shift_labels = labels[..., 1:].contiguous()\n",
                "            \n",
                "            loss = loss_fn(shift_logits.view(-1, config.vocab_size), shift_labels.view(-1))\n",
                "        \n",
                "        # Optimization: Scaled Backward Pass\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        steps += 1\n",
                "        \n",
                "        if steps % 100 == 0:\n",
                "            print(f\"Step {steps}: Loss {loss.item()}\")\n",
                "            \n",
                "        if steps % 500 == 0:\n",
                "            print(f\"\\n--- Step {steps} Generation ---\")\n",
                "            generate_text(model, tokenizer)\n",
                "            print(\"-----------------------------\\n\")\n",
                "\n",
                "# Save Checkpoint\n",
                "torch.save(model.state_dict(), save_path)\n",
                "print(f\"Checkpoint saved to {save_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Resume Training\n",
                "Now we simulate stopping and resuming by loading the checkpoint and training for 50 more steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Resuming training...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\sidhe\\AppData\\Local\\Temp\\ipykernel_23008\\3952903180.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
                        "  model.load_state_dict(torch.load(save_path))\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Resume Step 10: Loss 0.14603516459465027\n",
                        "Resume Step 20: Loss 0.12318285554647446\n",
                        "Resume Step 30: Loss 0.08086450397968292\n",
                        "Resume Step 40: Loss 0.11517078429460526\n",
                        "Resume Step 50: Loss 0.13376004993915558\n",
                        "\n",
                        "--- Step 5050 Generation ---\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\sidhe\\AppData\\Local\\Temp\\ipykernel_23008\\2461595262.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
                        "  with torch.cuda.amp.autocast():\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generated: First Citizen:\n",
                        "Before we proceed any further, hear me speak.\n",
                        "\n",
                        "All:\n",
                        "Speak, speak.\n",
                        "\n",
                        "First Citizen:\n",
                        "You are all resolved rather to die than to famish?\n",
                        "\n",
                        "All:\n",
                        "Resolved. resolved.\n",
                        "\n",
                        "First Citizen:\n",
                        "First, you know Caius Marcius is chief enemy to the people.\n",
                        "\n",
                        "All:\n",
                        "We know't, we know't.\n",
                        "\n",
                        "First Citizen:\n",
                        "Let us kill him, and we'll have corn at our own price.\n",
                        "Is't a verdict?\n",
                        "\n",
                        "All:\n",
                        "No more talking on't: let it be done: away, away!\n",
                        "\n",
                        "Second Citizen:\n",
                        "One word.\n",
                        "\n",
                        "First Citizen:\n",
                        "We are coniled and we will.\n",
                        "\n",
                        "MENENIUS:\n",
                        "Ay, we'll show 'em good friends,' we'll show themselves:\n",
                        "'Tis very strangely.\n",
                        "\n",
                        "Second Citizen:\n",
                        "And 'twas a purposed, we'll keep no tears;\n",
                        "But we shall have vengeance and meet the belly;\n",
                        "Where, they shall stand, will we do good men, we'll\n",
                        "With our senators: we'll promption 'O' good,'\n",
                        "Their very wayward.\n",
                        "\n",
                        "MENENIUS:\n",
                        "What we will, to do\n",
                        "Our Rome will in justice w: the gods do o' the city\n",
                        "In every mattling aparted, is as an art,\n",
                        "It cannot miss't: the other lady,\n",
                        "In peace what it has, in a little way, a kind of it is his country:\n",
                        "He's in a few, and his nobility, he waves mein it in his country, he'll carry it off.\n",
                        "What think you, say, think you?\n",
                        "\n",
                        "First Citizen:\n",
                        "Our Rome are our garments, the very he has our voices;\n",
                        "He would have offices, so it is as easy next, were as an a good will, he were a\n",
                        "painty.\n",
                        "\n",
                        "MENENIUS:\n",
                        "Sincts, we were a mind of fashion, they stay:\n",
                        "Why then we do we were as an a man could't be his\n",
                        "In hand, if he were not much beyond a good counsel, it is a\n",
                        "ity to the vivery keeps his capit.\n",
                        "\n",
                        "Second Citizen:\n",
                        "He's brows: go to-night, sir, it doth not go: he is as another merry as those that he were a noble man, one poor master?\n",
                        "\n",
                        "First Citizen:\n",
                        " their tongues speak: who's heifths upon his countrymen as they were as they were a panteveter'd;' for they are very strange, I wonder will a very little grave:\n",
                        "Silence that we have honours that, if't please you can he pass'd,\n",
                        "'Farewell: I'll peize him home, that he let me desire some good belly toadovery.\n",
                        "\n",
                        "Lone I can bury tauntedeem him to-morrow.\n",
                        "'Tis well, sir, was God's well meteads;\n",
                        "His very promise pass'd: his countrymen stay awhile,\n",
                        "'Twere an eye of his countrymen, they were as they were i' the senators are you wot'd, no queen,\n",
                        "How he were hanged, I did it in an answer.\n",
                        "\n",
                        "First Citizen:\n",
                        "Gardeliused your voices: he has a pretty master?\n",
                        "\n",
                        "Second Citizen:\n",
                        "And, sir, in what cannot think.\n",
                        "\n",
                        "BRUTUS:\n",
                        "What is the gods will he'll show thee by you a massies them a mould manifth, I will draw not that he pass'd:\n",
                        "Like death to Polixenes, that it doth henew the superflow'd again, the VolscesinguriedI' the peace of his countrymen,\n",
                        "'Twere a perfect woman moved you:\n",
                        "What think he was said, is it kindly leak before his countrymen with rushireet: if it did but grief he did it, I'll pray you, in it in an actorough case lost for a painting, you have seen a bust of rathematica, he were a\n",
                        "meticolariarried Nicanifth us, then he's no more but He', I warrant, and then; for he were as noble, I hear say,\n",
                        "Not to myself, that doubt but me a-proper:\n",
                        "Glower as they were by rage to behold, he's a pock will not work,\n",
                        "Even to the senate, the very pretty fooling,\n",
                        "In branish thing, he was like a man well, art a little grave sir, was his son, he would have been an angry with no warrant from the one,\n",
                        " a foul son, is it skill'd: he has a dear-five for the city is grown poor sir,\n",
                        "Scarson\n",
                        "-----------------------------\n",
                        "\n",
                        "Checkpoint saved to checkpoint_5050.pt\n",
                        "Resumed training completed.\n"
                    ]
                }
            ],
            "source": [
                "print(\"Resuming training...\")\n",
                "model = SmolLM2(config).to(device)\n",
                "model.load_state_dict(torch.load(save_path))\n",
                "model.train()\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
                "\n",
                "extra_steps = 50\n",
                "current_step = 0\n",
                "\n",
                "# Re-create dataloader for resume (in real scenario you'd want to skip already seen data)\n",
                "# Note: In this simple example, we restart the dataloader from the beginning.\n",
                "# In a real resume scenario, you'd want to save the dataloader state or skip 'steps' batches.\n",
                "train_dataloader_resume = DataLoader(lm_dataset, batch_size=4, shuffle=True)\n",
                "\n",
                "for batch in train_dataloader_resume:\n",
                "    if current_step >= extra_steps:\n",
                "        break\n",
                "        \n",
                "    input_ids = batch[\"input_ids\"].to(device)\n",
                "    labels = input_ids.clone()\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    logits = model(input_ids)\n",
                "    \n",
                "    shift_logits = logits[..., :-1, :].contiguous()\n",
                "    shift_labels = labels[..., 1:].contiguous()\n",
                "    \n",
                "    loss = loss_fn(shift_logits.view(-1, config.vocab_size), shift_labels.view(-1))\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    current_step += 1\n",
                "    if current_step % 10 == 0:\n",
                "        print(f\"Resume Step {current_step}: Loss {loss.item()}\")\n",
                "\n",
                "print(f\"\\n--- Step {steps + extra_steps} Generation ---\")\n",
                "generate_text(model, tokenizer, prompt=\"First Citizen\", max_new_tokens=1024)\n",
                "print(\"-----------------------------\\n\")\n",
                "\n",
                "torch.save(model.state_dict(), f\"checkpoint_{steps + extra_steps}.pt\")\n",
                "print(f\"Checkpoint saved to checkpoint_{steps + extra_steps}.pt\")\n",
                "\n",
                "print(\"Resumed training completed.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
