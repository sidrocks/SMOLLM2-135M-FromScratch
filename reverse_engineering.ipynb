{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Reverse Engineering SmolLM2-135M\n",
                "\n",
                "This notebook guides you through downloading the SmolLM2-135M model, inspecting its architecture, implementing it from scratch in PyTorch, and validating the implementation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Force reinstall torch and torchvision to fix version mismatch\n",
                "!pip uninstall -y torch torchvision\n",
                "!pip install torch torchvision transformers datasets huggingface_hub"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> **IMPORTANT**: After running the cell above, you **MUST** restart the Jupyter Kernel for the changes to take effect. Go to **Kernel > Restart Kernel** in the menu."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
                "import math\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Download and Load Model\n",
                "We load the pre-trained model from HuggingFace."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model Config:\n",
                        "LlamaConfig {\n",
                        "  \"architectures\": [\n",
                        "    \"LlamaForCausalLM\"\n",
                        "  ],\n",
                        "  \"attention_bias\": false,\n",
                        "  \"attention_dropout\": 0.0,\n",
                        "  \"bos_token_id\": 0,\n",
                        "  \"dtype\": \"bfloat16\",\n",
                        "  \"eos_token_id\": 0,\n",
                        "  \"head_dim\": 64,\n",
                        "  \"hidden_act\": \"silu\",\n",
                        "  \"hidden_size\": 576,\n",
                        "  \"initializer_range\": 0.041666666666666664,\n",
                        "  \"intermediate_size\": 1536,\n",
                        "  \"is_llama_config\": true,\n",
                        "  \"max_position_embeddings\": 8192,\n",
                        "  \"mlp_bias\": false,\n",
                        "  \"model_type\": \"llama\",\n",
                        "  \"num_attention_heads\": 9,\n",
                        "  \"num_hidden_layers\": 30,\n",
                        "  \"num_key_value_heads\": 3,\n",
                        "  \"pretraining_tp\": 1,\n",
                        "  \"rms_norm_eps\": 1e-05,\n",
                        "  \"rope_interleaved\": false,\n",
                        "  \"rope_scaling\": null,\n",
                        "  \"rope_theta\": 100000,\n",
                        "  \"tie_word_embeddings\": true,\n",
                        "  \"transformers_version\": \"4.57.1\",\n",
                        "  \"use_cache\": true,\n",
                        "  \"vocab_size\": 49152\n",
                        "}\n",
                        "\n",
                        "\n",
                        "Model Structure:\n",
                        "LlamaForCausalLM(\n",
                        "  (model): LlamaModel(\n",
                        "    (embed_tokens): Embedding(49152, 576)\n",
                        "    (layers): ModuleList(\n",
                        "      (0-29): 30 x LlamaDecoderLayer(\n",
                        "        (self_attn): LlamaAttention(\n",
                        "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
                        "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
                        "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
                        "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
                        "        )\n",
                        "        (mlp): LlamaMLP(\n",
                        "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
                        "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
                        "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
                        "          (act_fn): SiLUActivation()\n",
                        "        )\n",
                        "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
                        "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
                        "      )\n",
                        "    )\n",
                        "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
                        "    (rotary_emb): LlamaRotaryEmbedding()\n",
                        "  )\n",
                        "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "hf_model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
                "hf_config = AutoConfig.from_pretrained(model_id)\n",
                "\n",
                "print(\"Model Config:\")\n",
                "print(hf_config)\n",
                "print(\"\\nModel Structure:\")\n",
                "print(hf_model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Reverse Engineering Architecture\n",
                "Based on the config and printout, we can see it uses a Llama-style architecture:\n",
                "- **RMSNorm** for normalization.\n",
                "- **Rotary Positional Embeddings (RoPE)**.\n",
                "- **SwiGLU** activation in the MLP.\n",
                "- **Grouped Query Attention (GQA)** (though for 135M it might be standard MHA, let's check `num_key_value_heads`).\n",
                "\n",
                "Let's define the model from scratch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "\n",
                "class RMSNorm(nn.Module):\n",
                "    def __init__(self, dim, eps=1e-5):\n",
                "        super().__init__()\n",
                "        self.eps = eps\n",
                "        self.weight = nn.Parameter(torch.ones(dim))\n",
                "\n",
                "    def forward(self, x):\n",
                "        mean_square = (x.pow(2).mean(-1, keepdim=True))\n",
                "        x = x * torch.rsqrt(mean_square + self.eps)\n",
                "        return self.weight * x\n",
                "\n",
                "def rotate_half(x):\n",
                "    # Rotates half the hidden dims of the input.\n",
                "    x1 = x[..., : x.shape[-1] // 2]\n",
                "    x2 = x[..., x.shape[-1] // 2 :]\n",
                "    return torch.cat((-x2, x1), dim=-1)\n",
                "\n",
                "def apply_rotary_pos_emb(q, k, cos, sin):\n",
                "    # q, k: [bsz, heads, seq_len, head_dim]\n",
                "    # cos, sin: [seq_len, head_dim] -> unsqueeze to [1, 1, seq_len, head_dim]\n",
                "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
                "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
                "    \n",
                "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
                "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
                "    return q_embed, k_embed\n",
                "\n",
                "class MLP(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
                "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
                "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
                "        self.act_fn = nn.SiLU()\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
                "\n",
                "class Attention(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.hidden_size = config.hidden_size\n",
                "        self.num_heads = config.num_attention_heads\n",
                "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
                "        self.num_key_value_heads = config.num_key_value_heads\n",
                "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
                "        \n",
                "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
                "        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
                "        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
                "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.hidden_size, bias=False)\n",
                "\n",
                "    def forward(self, x, cos, sin, mask=None):\n",
                "        bsz, seq_len, _ = x.shape\n",
                "        q = self.q_proj(x).view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
                "        k = self.k_proj(x).view(bsz, seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
                "        v = self.v_proj(x).view(bsz, seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
                "        \n",
                "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
                "        \n",
                "        k = k.repeat_interleave(self.num_key_value_groups, dim=1)\n",
                "        v = v.repeat_interleave(self.num_key_value_groups, dim=1)\n",
                "        \n",
                "        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
                "        if mask is not None:\n",
                "            attn_weights = attn_weights + mask\n",
                "            \n",
                "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
                "        output = torch.matmul(attn_weights, v)\n",
                "        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
                "        return self.o_proj(output)\n",
                "\n",
                "class Block(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.self_attn = Attention(config)\n",
                "        self.mlp = MLP(config)\n",
                "        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
                "        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
                "\n",
                "    def forward(self, x, cos, sin, mask=None):\n",
                "        h = x + self.self_attn(self.input_layernorm(x), cos, sin, mask)\n",
                "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
                "        return out\n",
                "\n",
                "class SmolLM2(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.config = config\n",
                "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
                "        self.layers = nn.ModuleList([Block(config) for _ in range(config.num_hidden_layers)])\n",
                "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
                "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
                "        \n",
                "        # RoPE setup\n",
                "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
                "        self.rope_theta = getattr(config, \"rope_theta\", 10000.0)\n",
                "        self.inv_freq = 1.0 / (self.rope_theta ** (torch.arange(0, self.head_dim, 2).float() / self.head_dim))\n",
                "        self.max_pos = config.max_position_embeddings * 2\n",
                "        self._set_cos_sin_cache(self.max_pos)\n",
                "\n",
                "    def _set_cos_sin_cache(self, seq_len):\n",
                "        t = torch.arange(seq_len, dtype=torch.float32)\n",
                "        freqs = torch.outer(t, self.inv_freq)\n",
                "        emb = torch.cat((freqs, freqs), dim=-1)\n",
                "        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n",
                "        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n",
                "\n",
                "    def forward(self, input_ids):\n",
                "        bsz, seq_len = input_ids.shape\n",
                "        x = self.embed_tokens(input_ids)\n",
                "        \n",
                "        if self.cos_cached.device != x.device or self.cos_cached.shape[0] < seq_len:\n",
                "            self.inv_freq = self.inv_freq.to(x.device)\n",
                "            self._set_cos_sin_cache(max(seq_len, 2048))\n",
                "            \n",
                "        cos = self.cos_cached[:seq_len].to(dtype=x.dtype)\n",
                "        sin = self.sin_cached[:seq_len].to(dtype=x.dtype)\n",
                "        \n",
                "        mask = None\n",
                "        if seq_len > 1:\n",
                "            mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=input_ids.device)\n",
                "            mask = torch.triu(mask, diagonal=1)\n",
                "            \n",
                "        for layer in self.layers:\n",
                "            x = layer(x, cos, sin, mask)\n",
                "            \n",
                "        x = self.norm(x)\n",
                "        logits = self.lm_head(x)\n",
                "        return logits\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Weight Transfer\n",
                "Now we copy the weights from the HuggingFace model to our custom implementation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Weights copied successfully.\n"
                    ]
                }
            ],
            "source": [
                "custom_model = SmolLM2(hf_config).to(device)\n",
                "\n",
                "def copy_weights(hf_model, custom_model):\n",
                "    with torch.no_grad():\n",
                "        # Embeddings\n",
                "        custom_model.embed_tokens.weight.copy_(hf_model.model.embed_tokens.weight)\n",
                "        \n",
                "        # Layers\n",
                "        for i, (hf_layer, custom_layer) in enumerate(zip(hf_model.model.layers, custom_model.layers)):\n",
                "            custom_layer.input_layernorm.weight.copy_(hf_layer.input_layernorm.weight)\n",
                "            custom_layer.post_attention_layernorm.weight.copy_(hf_layer.post_attention_layernorm.weight)\n",
                "            \n",
                "            # Attention\n",
                "            custom_layer.self_attn.q_proj.weight.copy_(hf_layer.self_attn.q_proj.weight)\n",
                "            custom_layer.self_attn.k_proj.weight.copy_(hf_layer.self_attn.k_proj.weight)\n",
                "            custom_layer.self_attn.v_proj.weight.copy_(hf_layer.self_attn.v_proj.weight)\n",
                "            custom_layer.self_attn.o_proj.weight.copy_(hf_layer.self_attn.o_proj.weight)\n",
                "            \n",
                "            # MLP\n",
                "            custom_layer.mlp.gate_proj.weight.copy_(hf_layer.mlp.gate_proj.weight)\n",
                "            custom_layer.mlp.up_proj.weight.copy_(hf_layer.mlp.up_proj.weight)\n",
                "            custom_layer.mlp.down_proj.weight.copy_(hf_layer.mlp.down_proj.weight)\n",
                "            \n",
                "        # Final Norm and Head\n",
                "        custom_model.norm.weight.copy_(hf_model.model.norm.weight)\n",
                "        custom_model.lm_head.weight.copy_(hf_model.lm_head.weight)\n",
                "\n",
                "copy_weights(hf_model, custom_model)\n",
                "print(\"Weights copied successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Validation\n",
                "We verify that the outputs match."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Max difference: 4.1961669921875e-05\n",
                        "Validation Passed!\n"
                    ]
                }
            ],
            "source": [
                "input_text = \"Once upon a time\"\n",
                "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    hf_outputs = hf_model(**inputs).logits\n",
                "    custom_outputs = custom_model(inputs.input_ids)\n",
                "\n",
                "diff = (hf_outputs - custom_outputs).abs().max()\n",
                "print(f\"Max difference: {diff.item()}\")\n",
                "\n",
                "assert diff < 1e-4, \"Models diverge!\"\n",
                "print(\"Validation Passed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Export Model\n",
                "We save the model class to `model.py` for use in training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "model.py created.\n"
                    ]
                }
            ],
            "source": [
                "code = \"\"\"\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "\n",
                "class RMSNorm(nn.Module):\n",
                "    def __init__(self, dim, eps=1e-5):\n",
                "        super().__init__()\n",
                "        self.eps = eps\n",
                "        self.weight = nn.Parameter(torch.ones(dim))\n",
                "\n",
                "    def forward(self, x):\n",
                "        mean_square = (x.pow(2).mean(-1, keepdim=True))\n",
                "        x = x * torch.rsqrt(mean_square + self.eps)\n",
                "        return self.weight * x\n",
                "\n",
                "def rotate_half(x):\n",
                "    # Rotates half the hidden dims of the input.\n",
                "    x1 = x[..., : x.shape[-1] // 2]\n",
                "    x2 = x[..., x.shape[-1] // 2 :]\n",
                "    return torch.cat((-x2, x1), dim=-1)\n",
                "\n",
                "def apply_rotary_pos_emb(q, k, cos, sin):\n",
                "    # q, k: [bsz, heads, seq_len, head_dim]\n",
                "    # cos, sin: [seq_len, head_dim] -> unsqueeze to [1, 1, seq_len, head_dim]\n",
                "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
                "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
                "    \n",
                "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
                "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
                "    return q_embed, k_embed\n",
                "\n",
                "class MLP(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
                "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
                "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
                "        self.act_fn = nn.SiLU()\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
                "\n",
                "class Attention(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.hidden_size = config.hidden_size\n",
                "        self.num_heads = config.num_attention_heads\n",
                "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
                "        self.num_key_value_heads = config.num_key_value_heads\n",
                "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
                "        \n",
                "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
                "        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
                "        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
                "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.hidden_size, bias=False)\n",
                "\n",
                "    def forward(self, x, cos, sin, mask=None):\n",
                "        bsz, seq_len, _ = x.shape\n",
                "        q = self.q_proj(x).view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
                "        k = self.k_proj(x).view(bsz, seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
                "        v = self.v_proj(x).view(bsz, seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
                "        \n",
                "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
                "        \n",
                "        k = k.repeat_interleave(self.num_key_value_groups, dim=1)\n",
                "        v = v.repeat_interleave(self.num_key_value_groups, dim=1)\n",
                "        \n",
                "        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
                "        if mask is not None:\n",
                "            attn_weights = attn_weights + mask\n",
                "            \n",
                "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
                "        output = torch.matmul(attn_weights, v)\n",
                "        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
                "        return self.o_proj(output)\n",
                "\n",
                "class Block(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.self_attn = Attention(config)\n",
                "        self.mlp = MLP(config)\n",
                "        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
                "        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
                "\n",
                "    def forward(self, x, cos, sin, mask=None):\n",
                "        h = x + self.self_attn(self.input_layernorm(x), cos, sin, mask)\n",
                "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
                "        return out\n",
                "\n",
                "class SmolLM2(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.config = config\n",
                "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
                "        self.layers = nn.ModuleList([Block(config) for _ in range(config.num_hidden_layers)])\n",
                "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
                "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
                "        \n",
                "        # RoPE setup\n",
                "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
                "        self.rope_theta = getattr(config, \"rope_theta\", 10000.0)\n",
                "        self.inv_freq = 1.0 / (self.rope_theta ** (torch.arange(0, self.head_dim, 2).float() / self.head_dim))\n",
                "        self.max_pos = config.max_position_embeddings * 2\n",
                "        self._set_cos_sin_cache(self.max_pos)\n",
                "\n",
                "    def _set_cos_sin_cache(self, seq_len):\n",
                "        t = torch.arange(seq_len, dtype=torch.float32)\n",
                "        freqs = torch.outer(t, self.inv_freq)\n",
                "        emb = torch.cat((freqs, freqs), dim=-1)\n",
                "        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n",
                "        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n",
                "\n",
                "    def forward(self, input_ids):\n",
                "        bsz, seq_len = input_ids.shape\n",
                "        x = self.embed_tokens(input_ids)\n",
                "        \n",
                "        if self.cos_cached.device != x.device or self.cos_cached.shape[0] < seq_len:\n",
                "            self.inv_freq = self.inv_freq.to(x.device)\n",
                "            self._set_cos_sin_cache(max(seq_len, 2048))\n",
                "            \n",
                "        cos = self.cos_cached[:seq_len].to(dtype=x.dtype)\n",
                "        sin = self.sin_cached[:seq_len].to(dtype=x.dtype)\n",
                "        \n",
                "        mask = None\n",
                "        if seq_len > 1:\n",
                "            mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=input_ids.device)\n",
                "            mask = torch.triu(mask, diagonal=1)\n",
                "            \n",
                "        for layer in self.layers:\n",
                "            x = layer(x, cos, sin, mask)\n",
                "            \n",
                "        x = self.norm(x)\n",
                "        logits = self.lm_head(x)\n",
                "        return logits\n",
                "\"\"\"\n",
                "\n",
                "with open(\"model.py\", \"w\") as f:\n",
                "    f.write(code)\n",
                "    \n",
                "print(\"model.py created.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
